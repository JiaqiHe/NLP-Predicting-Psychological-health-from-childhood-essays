{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalize and Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read essays, depression and anxiety from different files and concatenate them into a dictionary containing all info needed for further exploration.\n",
    "\n",
    "For this jupyter notebook to run, download docker image by executing the command line\n",
    "```bash\n",
    "sudo docker pull pupster90/cse255-18\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:36:26.172589Z",
     "start_time": "2018-06-12T19:36:24.007525Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install gensim scikit-plot nltk keras tqdm\n",
    "#############################################\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import scikitplot\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import gensim\n",
    "import re, collections\n",
    "import string\n",
    "import scikitplot.plotters as skplt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:36:27.708816Z",
     "start_time": "2018-06-12T19:36:27.627656Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:36:31.326456Z",
     "start_time": "2018-06-12T19:36:30.804377Z"
    }
   },
   "outputs": [],
   "source": [
    "def words(text):\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "NWORDS = train(words(open('big.txt').read()))\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [a + b[1:] for a, b in splits if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "    replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "    inserts = [a + c + b for a, b in splits for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words):\n",
    "    return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:36:32.516493Z",
     "start_time": "2018-06-12T19:36:32.495807Z"
    }
   },
   "outputs": [],
   "source": [
    "def modifyText(text):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    text = re.sub(r'\\[[a-zA-Z ]+\\]|\\r\\n', '', text)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    mod = ''\n",
    "    counter = 0\n",
    "    words_count = 0\n",
    "    for word in text.split(' '):\n",
    "        new_word = correct(word.lower())\n",
    "        if new_word not in sw:\n",
    "            mod += new_word + ' '\n",
    "            words_count += 1\n",
    "            if(new_word != word.lower()):\n",
    "                counter += 1\n",
    "    return mod, counter, words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:36:35.148011Z",
     "start_time": "2018-06-12T19:36:35.136032Z"
    }
   },
   "outputs": [],
   "source": [
    "def processFile(filename):\n",
    "    global data\n",
    "    f = open(filename)\n",
    "    with open(filename, 'rb') as f:\n",
    "        texts = []\n",
    "        for line in f:\n",
    "            texts.append(line.decode(errors='ignore'))\n",
    "        text = reduce(lambda x, y : x + y , texts[2:])\n",
    "        words = filename.split(\"-\")\n",
    "        key = words[-1].split(\".\")[0].upper()\n",
    "        data[key] = dict({\"essay\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T19:37:28.969550Z",
     "start_time": "2018-06-12T19:37:28.851234Z"
    }
   },
   "outputs": [],
   "source": [
    "def getData(data_filename, preprocess=True):\n",
    "    global data\n",
    "    global counters\n",
    "    global words_counts\n",
    "    if os.path.exists(data_filename):\n",
    "        with open(data_filename, 'rb') as fp:\n",
    "            data = pickle.load(fp)\n",
    "    else:\n",
    "        directory = \"./essays/rtf/\"\n",
    "        counter = 0\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".rtf\"):\n",
    "                processFile(directory + filename)\n",
    "                counter += 1\n",
    "        print(\"There are {0} essays.\".format(counter))\n",
    "        f = open(\"./277labels/labels.csv\")\n",
    "        complete_entries_list = []\n",
    "        for line in f:\n",
    "            entries = line.split(\",\")\n",
    "            key = entries[0]\n",
    "            depression = entries[1]\n",
    "            anxiety = entries[2]\n",
    "            if(key == \"ncdsid\" or depression == \"$null$\" or anxiety == \"$null$\"):\n",
    "                continue\n",
    "            else:\n",
    "                depression = float(depression)\n",
    "                anxiety = float(anxiety)\n",
    "                if(key not in data or depression == -1.0 or anxiety == -1.0):\n",
    "                    if key in data:\n",
    "                        deleted = data.pop(key)\n",
    "                    continue\n",
    "                else:\n",
    "                    data[key][\"anxiety\"] = anxiety\n",
    "                    data[key][\"depression\"] = depression\n",
    "                    counter += 1\n",
    "                    complete_entries_list.append(key)\n",
    "        print(\"There are {0} entries that have complete info.\".format(len(complete_entries_list)))\n",
    "        selected = dict({})\n",
    "        for key in complete_entries_list:\n",
    "            selected[key] = data[key]\n",
    "            selected[key][\"essay\"] = re.sub(r'[0-9]+[a-zA-Zx*]*\\s|words ', '', selected[key][\"essay\"])\n",
    "        if preprocess:\n",
    "            for i in tqdm(range(len(complete_entries_list))):\n",
    "                key = complete_entries_list[i]\n",
    "                selected[key]['essay'], counter, words_count = modifyText(selected[key]['essay'])\n",
    "                counters.append(counter)\n",
    "                words_counts.append(words_count)\n",
    "        with open(data_filename, 'wb') as fp:\n",
    "            pickle.dump(selected, fp, protocol=2)\n",
    "        data = selected\n",
    "    return data, counters, words_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load processed data from disk or process raw data here__\n",
    "\n",
    "One option is to load vector form of data.\n",
    "\n",
    "Here, getData(filename) would first search if such file exists, if not, it will then collect raw data, preprocess it, you can select to do correction by setting preprocess=True(default)/False.\n",
    "\n",
    "clarifications about files:\n",
    "* data_embedding_google.p: processed essay Word2Vec embeddings (vectors)\n",
    "* data_mod_contain_corrections.p: processed essay Word2Vec embeddings with correction ratio and total number of words (vectors)\n",
    "* data_modified.p: processed essay (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another choice is to use text files. \n",
    "Here we use \"data_modified.p\" to set `data`.\n",
    "Data are stored in a dictionary named as `data`. The UCDSId is used to serve as the key of this dictionary, and the value is the detailed info encapsulated in a dictionary containing __essay__, __depression__, and __anxiety__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:40:16.304126Z",
     "start_time": "2018-06-12T20:40:16.263096Z"
    }
   },
   "outputs": [],
   "source": [
    "data = dict({})\n",
    "counters = []\n",
    "words_counts = []\n",
    "data, counters, words_counts = getData('data_modified.p')\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df = df.dropna()\n",
    "depression = df['depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:41:37.895293Z",
     "start_time": "2018-06-12T20:41:37.872132Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, depression, test_size = 0.2, random_state=49)\n",
    "y_test = [1 if x > threshold else 0 for x in y_test.values.tolist()]\n",
    "y_train = [1 if x > threshold else 0 for x in y_train.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:22:08.154567Z",
     "start_time": "2018-06-12T19:37:30.959217Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = dict({})\n",
    "counters = []\n",
    "words_counts = []\n",
    "data, counters, words_counts = getData('data_mod_contain_corrections.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:58:26.567679Z",
     "start_time": "2018-06-01T03:58:26.560066Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", tokenizer=nltk.word_tokenize,\n",
    "    preprocessor=None, stop_words='english', max_features=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:58:34.174295Z",
     "start_time": "2018-06-01T03:58:30.092461Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bag_of_words = count_vectorizer.fit_transform(df['essay'])\n",
    "print(\"There are {0} unique words in corpus.\".format(len(count_vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T03:58:39.398131Z",
     "start_time": "2018-06-01T03:58:38.095381Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)\n",
    "truncated_bag_of_words = svd.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Word2Vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T18:39:45.073597Z",
     "start_time": "2018-06-08T18:39:45.042360Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec  \n",
    "    Args:\n",
    "        sentences: iterator for sentences    \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=500, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train Word2Vec model and save to local disk.\n",
    "\n",
    "filename clarifications:\n",
    "\n",
    "* w2vmodel_modified: trained word2vec model using processed data (containing numbers), embedding size 100\n",
    "* w2vmodel_modified_1000: trained word2vec model using processed data (containing numbers), embedding size 1000\n",
    "* w2vmodel_modified_delete_numbers: trained word2vec model using processed data (without numbers), embedding size 100\n",
    "* w2vmodel_modified_delete_numbers_500: trained word2vec model using processed data (without numbers), embedding size 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T17:48:11.059130Z",
     "start_time": "2018-06-07T17:48:10.304267Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "w2vec = get_word2vec(MySentences(X_train.values, X_test.values),\n",
    "                     'w2vmodel_modified_delete_numbers_500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another option is to load Google's pre-trained Word2Vec model.\n",
    "Before running the following block, download the Google model https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "w2vec = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Having a word2vec model, we then use it to transform each documents into a feature vector. In order to convert a document of multiple words into a single vector using trained word2vec, we take the word2vec of all words in the document, then take its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T17:48:18.407339Z",
     "start_time": "2018-06-07T17:48:18.355031Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-31T23:55:33.671378Z",
     "start_time": "2018-05-31T23:27:27.376Z"
    },
    "hidden": true
   },
   "source": [
    "Transform training data to `mean_embedded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T17:48:27.077799Z",
     "start_time": "2018-06-07T17:48:21.118748Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "mean_embedded = mean_embedding_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T17:48:30.611642Z",
     "start_time": "2018-06-07T17:48:29.204713Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_embedded = mean_embedding_vectorizer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calculate the cosine similarity\n",
    "\n",
    "Here to execute the code, you need to make sure `data` is the embedding of essays you want to use. Uncomment the first line to use vectors generated by google's word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:41:08.076364Z",
     "start_time": "2018-06-13T23:40:47.105613Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data, _, _ = getData('data_mod_contain_corrections.p')\n",
    "origin = data\n",
    "y_train = df['depression'].values.tolist()\n",
    "cos = cosine_similarity(origin)\n",
    "counter = np.array([0] * 400).reshape(20, 20)\n",
    "index_to_delete = set([])\n",
    "for i in range(len(cos)):\n",
    "    for j in range(i, len(cos[0])):\n",
    "        if cos[i][j] > 0.95 and i != j and y_train[i] != y_train[j]:\n",
    "            counter[int(max(y_train[i], y_train[j]))][int(min(y_train[i], y_train[j]))] += 1\n",
    "            index_to_delete.add(i)\n",
    "            index_to_delete.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T00:48:59.441783Z",
     "start_time": "2018-06-14T00:48:59.427327Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in counter:\n",
    "    print(\"{0:5d}\\t{1:5d}\\t{2:5d}\\t{3:5d}\\t{4:5d}\\t{5:5d}\\t{6:5d}\\t{7:5d}\\t{8:5d}\\t{9:5d}\".format(i[0], i[1], i[2],\\\n",
    "                                                                                                     i[3],i[4],i[5],\\\n",
    "                                                                                                     i[6],i[7],i[8],i[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:41:10.404547Z",
     "start_time": "2018-06-13T23:41:10.163278Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('# of pairs')\n",
    "plt.imshow(counter[:12,:12], cmap=\"GnBu\")\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])\n",
    "cax.get_xaxis().set_visible(False)\n",
    "cax.get_yaxis().set_visible(False)\n",
    "cax.patch.set_alpha(0.3)\n",
    "cax.set_frame_on(False)\n",
    "plt.colorbar(orientation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "change selected index's labels to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:44:15.446775Z",
     "start_time": "2018-06-13T23:44:15.439683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ori_label = df['depression'].values.tolist()\n",
    "filtered_label = [ori_label[i] if (i not in index_to_delete) else -1 for i in range(len(ori_label))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "delete selected indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:37:27.282738Z",
     "start_time": "2018-06-13T23:37:27.262504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filtered_data = []\n",
    "for i in range(len(data)):\n",
    "    if(i not in index_to_delete):\n",
    "        filtered_data.append(data[i])\n",
    "filtered_data = np.array(filtered_data)\n",
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:37:29.471594Z",
     "start_time": "2018-06-13T23:37:29.464151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ori_label = df['depression'].values.tolist()\n",
    "filtered_label = [ori_label[i] for i in range(len(ori_label)) if i not in index_to_delete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:44:38.495513Z",
     "start_time": "2018-06-13T23:44:38.466794Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, filtered_label, test_size = 0.2, random_state=49)\n",
    "y_test = [1 if x > threshold else x for x in y_test]\n",
    "y_train = [1 if x > threshold else x for x in y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:44:46.665617Z",
     "start_time": "2018-06-13T23:44:46.652961Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=15, verbose=1, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:46:18.872535Z",
     "start_time": "2018-06-13T23:44:48.171382Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:46:55.162071Z",
     "start_time": "2018-06-13T23:46:54.612734Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:46:58.355039Z",
     "start_time": "2018-06-13T23:46:58.351140Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = [ x==y for (x,y) in zip(pred.tolist(), y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:47:01.085195Z",
     "start_time": "2018-06-13T23:47:01.080275Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(a)/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:47:05.235744Z",
     "start_time": "2018-06-13T23:47:03.362557Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:54:34.093743Z",
     "start_time": "2018-06-13T23:54:32.987017Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(X_test).tolist()\n",
    "pred_indices = np.argmax(probas, axis=1)\n",
    "# classes = np.array(range(clf.n_classes_))\n",
    "classes = np.array([-1,0,1])\n",
    "preds = classes[pred_indices]\n",
    "# true_labels = [int(x+1) for x in y_test]\n",
    "# print('Log loss: {}'.format(log_loss(classes[y_test], probas)))\n",
    "# print('Accuracy: {}'.format(accuracy_score(classes[y_test], preds)))\n",
    "skplt.plot_confusion_matrix(y_test, preds)\n",
    "# skplt.plot_confusion_matrix(true_labels, preds)\n",
    "probas = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T21:30:52.021361Z",
     "start_time": "2018-06-13T21:30:51.740500Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scikitplot.metrics.plot_precision_recall(y_test, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T21:30:55.330792Z",
     "start_time": "2018-06-13T21:30:55.058988Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scikitplot.metrics.plot_roc(y_test, probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:44:42.639710Z",
     "start_time": "2018-06-12T20:44:42.626467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def xgboost_plst():\n",
    "    param = {}\n",
    "    param['max_depth']= 2   # depth of tree\n",
    "    param['eta'] = 0.3      # shrinkage parameter\n",
    "    param['silent'] = 1     # not silent\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param['nthread'] = 7 # Number of threads used\n",
    "    param['eval_metric'] = 'logloss'\n",
    "\n",
    "    plst = param.items()\n",
    "    return plst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:00:54.804455Z",
     "start_time": "2018-06-13T22:00:39.019899Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_embedded = X_train\n",
    "test_embedded = X_test\n",
    "dtrain = xgb.DMatrix(mean_embedded, label=np.array(y_train))\n",
    "dtest = xgb.DMatrix(test_embedded, label=np.array(y_test))\n",
    "booster = xgb.train(xgboost_plst(), dtrain, 400)\n",
    "pred = booster.predict(dtest, output_margin=False, ntree_limit=booster.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-01T22:37:31.377536Z",
     "start_time": "2018-06-01T22:37:30.347024Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(booster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:45:03.947436Z",
     "start_time": "2018-06-12T20:45:03.942853Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = [(x - 0.5)*(y-0.5) > 0 for (x, y) in zip(pred.tolist(), y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:45:05.060531Z",
     "start_time": "2018-06-12T20:45:05.054337Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum(a)/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:45:07.745389Z",
     "start_time": "2018-06-12T20:45:07.529011Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred_indices = [1 if x < 0 else 0 for x in pred]\n",
    "# print('Log loss: {}'.format(log_loss(classes[y_test], pred)))\n",
    "# print('Accuracy: {}'.format(accuracy_score(classes[y_test], preds)))\n",
    "skplt.plot_confusion_matrix(y_test, pred_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T00:05:31.122479Z",
     "start_time": "2018-06-13T23:57:11.236681Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_embedded = X_train\n",
    "test_embedded = X_test\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, class_weight='balanced'), algorithm=\"SAMME.R\", n_estimators=1000)\n",
    "clf = clf.fit(mean_embedded, y_train)\n",
    "pred = clf.predict(test_embedded)\n",
    "a = [ x==y for (x,y) in zip(pred.tolist(), y_test)]\n",
    "print(sum(a)/len(a))\n",
    "probas = clf.predict_proba(test_embedded).tolist()\n",
    "pred_indices = np.argmax(probas, axis=1)\n",
    "# classes = np.array(range(0, 2))\n",
    "classes = np.array([-1,0,1])\n",
    "preds = classes[pred_indices]\n",
    "print('Log loss: {}'.format(log_loss(y_test, probas)))\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, preds)))\n",
    "skplt.plot_confusion_matrix(y_test, preds)\n",
    "# print('Log loss: {}'.format(log_loss(classes[y_test], probas)))\n",
    "# print('Accuracy: {}'.format(accuracy_score(classes[y_test], preds)))\n",
    "# skplt.plot_confusion_matrix(classes[y_test], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T21:03:01.682494Z",
     "start_time": "2018-06-12T21:02:59.855604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf.score(mean_embedded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:10:56.756382Z",
     "start_time": "2018-06-13T22:10:56.537538Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skplt.plot_confusion_matrix(classes[y_test], preds, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:11:10.189694Z",
     "start_time": "2018-06-13T22:11:09.671812Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scikitplot.metrics.plot_precision_recall(y_test, probas)\n",
    "scikitplot.metrics.plot_roc(y_test, probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T03:07:00.383419Z",
     "start_time": "2018-06-06T03:06:59.928906Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat nn_config.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:45:35.252059Z",
     "start_time": "2018-06-12T20:45:35.247926Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:45:36.499695Z",
     "start_time": "2018-06-12T20:45:36.493082Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Pretty Print\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:45:38.911238Z",
     "start_time": "2018-06-12T20:45:38.899994Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "def import_config():\n",
    "    with open(\"nn_config.yaml\", 'r') as ymlfile:\n",
    "        try:\n",
    "            cfg = yaml.load(ymlfile)\n",
    "        except yaml.YAMLError as err:\n",
    "            print(err)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:45:40.410903Z",
     "start_time": "2018-06-12T20:45:40.390271Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cfg = import_config()\n",
    "\n",
    "## Is it loaded correctly?\n",
    "pp.pprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:45:43.169876Z",
     "start_time": "2018-06-12T20:45:43.163169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    return (data - mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T02:49:07.731410Z",
     "start_time": "2018-06-14T02:49:07.728504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_data = mean_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:19:25.199845Z",
     "start_time": "2018-06-14T03:19:25.178570Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learning_rate = cfg['learning_rate']\n",
    "training_epochs = cfg['epochs'] * 10\n",
    "train_valid_split = cfg['training_to_validation_ratio']\n",
    "num_batches = cfg['num_mini_batches']\n",
    "display_step = cfg['display_step'] * 10\n",
    "\n",
    "num_examples= training_data.shape[0]\n",
    "\n",
    "# The first `num_train_examples` should be used for training, the rest for validation.\n",
    "num_train_examples = int(num_examples * train_valid_split)\n",
    "\n",
    "batch_size = num_train_examples/num_batches\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = cfg['hidden_layer_sizes']['first_layer']*3 # 1st layer number of features\n",
    "n_hidden_2 = cfg['hidden_layer_sizes']['second_layer']*3 # 2nd layer number of features\n",
    "n_input = 302 # change input size here\n",
    "n_classes = 3 # change classes numbers here\n",
    "\n",
    "\n",
    "print(\"Total Training examples: %d, Number of Batches: %d, Batch Size: %d\" %(num_train_examples,num_batches,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:19:27.874105Z",
     "start_time": "2018-06-14T03:19:27.806336Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TF Graph input\n",
    "## Use the below placeholders appropriately inside the train_nn() function\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, 3])\n",
    "n_hidden_3 = 5\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:19:30.398048Z",
     "start_time": "2018-06-14T03:19:30.380557Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_feedforward_nn_model(x, weights, biases):\n",
    "    # Hidden layer with SIGMOID activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, 0.5)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    layer_2 = tf.nn.dropout(layer_2, 0.5)\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "    out_layer = tf.add(tf.matmul(layer_3, weights['out']), biases['out'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:19:32.856111Z",
     "start_time": "2018-06-14T03:19:32.535999Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "def stitch_network(x, y, weights, biases, learning_rate):\n",
    "    \n",
    "    pred_raw = create_feedforward_nn_model(x, weights, biases)\n",
    "    pred = tf.round(tf.nn.sigmoid(pred_raw))\n",
    "    probas = tf.nn.sigmoid(pred_raw)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred_raw, labels=y))\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    return pred_raw, pred, loss, train_op, probas\n",
    "\n",
    "\n",
    "pred_raw, pred, loss, train_op, probas = stitch_network(x, y, weights, biases, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:19:35.335082Z",
     "start_time": "2018-06-14T03:19:35.325997Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:19:42.509463Z",
     "start_time": "2018-06-14T03:19:42.332352Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_nn():\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "        \n",
    "        ## this is needed to print debug statements during training.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        x_train, x_valid = features[:num_train_examples], features[num_train_examples:]\n",
    "        y_train, y_valid = labels[:num_train_examples], labels[num_train_examples:]\n",
    "\n",
    "        y_valid = y_valid.reshape(len(y_valid), 1)\n",
    "        y_valid = np.array([[0, 0, 1] if x == -1 else ([0, 1, 0] if x == 0 else [1, 0, 0]) for x in y_valid])\n",
    "        training_losses = []\n",
    "        training_accs = []\n",
    "\n",
    "        validation_losses = []\n",
    "        validation_accs = []\n",
    "        \n",
    "        \n",
    "        for epoch in range(training_epochs):\n",
    "            loss_counter = 0\n",
    "            correct_counter = 0\n",
    "            for num_batch in range(num_batches):\n",
    "                cur_features = x_train[int(num_batch * batch_size) : int(min(len(x_train), (num_batch + 1) * batch_size))]\n",
    "                cur_labels = y_train[int(num_batch * batch_size) : int(min(len(x_train), (num_batch + 1) * batch_size))]\n",
    "                cur_labels = cur_labels.reshape(len(cur_labels), 1)\n",
    "                cur_labels = np.array([[0, 0, 1] if x == -1 else ([0, 1, 0] if x == 0 else [1, 0, 0]) for x in cur_labels])\n",
    "                _, cur_loss, cur_pred = sess.run([train_op, loss, pred], feed_dict={x:cur_features, y:cur_labels})\n",
    "                loss_counter += cur_loss * len(cur_labels)\n",
    "#                 correct_counter += sum([1 if x==y else 0 for (x,y) in zip(cur_labels, cur_pred)])\n",
    "#             training_losses.append(np.float64(loss_counter/len(x_train)))\n",
    "#             training_accs.append(np.float64(correct_counter/len(x_train)))\n",
    "            \n",
    "            _, val_loss, val_pred = sess.run([train_op, loss, pred], feed_dict={x: x_valid, y: y_valid})\n",
    "            validation_losses.append(np.float64(val_loss))\n",
    "#             validation_accs.append(np.float64(sum([1 if x==y else 0 for (x,y) in zip(y_valid, val_pred)])/len(y_valid)))\n",
    "        \n",
    "#             if epoch%display_step==0:\n",
    "#                     print(\"Epoch {0} | Tr loss: {1} | Tr accuracy {2} | Va loss: {3} | Va accuracy: {4}\"\\\n",
    "#                           .format(epoch + 1,training_losses[epoch],training_accs[epoch],validation_losses[epoch], validation_accs[epoch]))       \n",
    "#         print(\"Optimization Finished!\")\n",
    "\n",
    "        test_predictions = []\n",
    "#         test_label = np.array[0] * len(test_features)\n",
    "#         test_label = test_label\n",
    "        test_pred, proba = sess.run([pred, probas], feed_dict={x: test_features})\n",
    "        test_predictions = np.float64(test_pred.T[0]) \n",
    "        \n",
    "        ## this is needed to print debug statements during training.\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    \n",
    "    ## close TF session if open\n",
    "    if 'session' in locals() and sess is not None:\n",
    "        print('Close interactive session')\n",
    "        sess.close()\n",
    "        \n",
    "    return training_losses, validation_losses, training_accs, validation_accs, test_predictions, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:21:43.340021Z",
     "start_time": "2018-06-14T03:19:55.187963Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# features = mean_embedded\n",
    "# labels = np.array(y_train)\n",
    "features = X_train\n",
    "labels = np.array(y_train)\n",
    "test_features = X_test\n",
    "# test_features = test_embedded\n",
    "training_losses, validation_losses, training_accs, validation_accs, test_predictions, probas = train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:23:09.326632Z",
     "start_time": "2018-06-14T03:23:09.093390Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classes = np.array([1,0,-1])\n",
    "test_predictions = classes[np.argmax(probas, axis = 1)]\n",
    "skplt.plot_confusion_matrix(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:24:33.336495Z",
     "start_time": "2018-06-14T03:24:33.329539Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = [x == y for (x,y) in zip(y_test, test_predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:24:42.453963Z",
     "start_time": "2018-06-14T03:24:42.440084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sum(a)/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T03:11:58.126002Z",
     "start_time": "2018-06-14T03:11:58.115767Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:03.066089Z",
     "start_time": "2018-06-12T20:49:03.058714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "probas = [[x, 1-x] for x in probas]\n",
    "probas = [[x[0][0], x[1][0]] for x in probas]\n",
    "probas = np.array(probas)\n",
    "probas = probas.tolist()\n",
    "probas = [[x[1], x[0]] for x in probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:10.432461Z",
     "start_time": "2018-06-12T20:49:10.425984Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_vs_epochs(training_losses, validation_losses):\n",
    "    plt.title(\"loss vs epochs\")\n",
    "    plt.plot(training_losses)\n",
    "    plt.plot(validation_losses)\n",
    "    plt.legend([\"training\",\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:11.268430Z",
     "start_time": "2018-06-12T20:49:11.263694Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_acc_vs_epochs(training_acc, validation_acc):\n",
    "    plt.title(\"accuracy vs epochs\")\n",
    "    plt.plot(training_acc)\n",
    "    plt.plot(validation_acc)\n",
    "    plt.legend([\"training\",\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:12.953109Z",
     "start_time": "2018-06-12T20:49:12.944251Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plots(training_losses, validation_losses, training_accs, validation_accs):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plot_loss_vs_epochs(training_losses, validation_losses)\n",
    "    plt.subplot(1,2,2)\n",
    "    plot_acc_vs_epochs(training_accs, validation_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:17.260315Z",
     "start_time": "2018-06-12T20:49:16.646118Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots(training_losses, validation_losses, training_accs, validation_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:29.813345Z",
     "start_time": "2018-06-12T20:49:29.807422Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = [1 if x==y else 0 for (x,y) in zip(test_predictions.tolist(), y_test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:30.845920Z",
     "start_time": "2018-06-12T20:49:30.839971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sum(a)/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:33.004104Z",
     "start_time": "2018-06-12T20:49:32.807642Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skplt.plot_confusion_matrix(y_test, test_predictions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:40.092305Z",
     "start_time": "2018-06-12T20:49:39.854950Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scikitplot.metrics.plot_precision_recall(y_test, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T20:49:43.306474Z",
     "start_time": "2018-06-12T20:49:43.019014Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scikitplot.metrics.plot_roc(y_test, probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:47:01.431513Z",
     "start_time": "2018-06-13T22:47:01.409117Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use the Keras tokenizer\n",
    "num_words = 2000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train.values)\n",
    "# Pad the data \n",
    "X = tokenizer.texts_to_sequences(X_train.values)\n",
    "X = pad_sequences(X, maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:47:34.314155Z",
     "start_time": "2018-06-13T22:47:34.284536Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build out our simple LSTM\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "num_words = 2000\n",
    "# Model saving callback\n",
    "ckpt_callback = ModelCheckpoint('keras_model', \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim, input_length = X.shape[1]))\n",
    "model.add(LSTM(lstm_out, recurrent_dropout=0.5, dropout=0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['categorical_crossentropy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T02:49:45.581061Z",
     "start_time": "2018-06-06T02:49:45.510822Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Y = np.array(y_train)\n",
    "Y = to_categorical(np.array(y_train))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify=Y)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T21:43:28.267572Z",
     "start_time": "2018-06-05T21:15:08.532569Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "trained_model = model.fit(X_train, Y_train, epochs=3, batch_size=batch_size, validation_split=0.2, callbacks=[ckpt_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T02:49:55.324028Z",
     "start_time": "2018-06-06T02:49:52.930169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trained_model = load_model('keras_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T02:50:33.770581Z",
     "start_time": "2018-06-06T02:49:57.895302Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred = trained_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T02:53:00.282664Z",
     "start_time": "2018-06-06T02:50:35.785680Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_res = trained_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T21:45:21.814553Z",
     "start_time": "2018-06-05T21:45:21.806394Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T02:53:20.002166Z",
     "start_time": "2018-06-06T02:53:19.990369Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "probas = train_res\n",
    "pred_indices = np.argmax(probas, axis=1)\n",
    "classes = np.array(range(2))\n",
    "preds = classes[pred_indices]\n",
    "accuracy_score(classes[np.argmax(Y_train, axis=1)], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T02:53:24.086308Z",
     "start_time": "2018-06-06T02:53:23.839727Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "probas = pred\n",
    "pred_indices = np.argmax(probas, axis=1)\n",
    "classes = np.array(range(2))\n",
    "preds = classes[pred_indices]\n",
    "print('Log loss: {}'.format(log_loss(classes[np.argmax(Y_test, axis=1)], probas)))\n",
    "print('Accuracy: {}'.format(accuracy_score(classes[np.argmax(Y_test, axis=1)], preds)))\n",
    "skplt.plot_confusion_matrix(classes[np.argmax(Y_test, axis=1)], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T01:42:37.333287Z",
     "start_time": "2018-06-06T01:42:37.063011Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scikitplot.metrics.plot_precision_recall(Y_test[:,1], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T01:43:17.337527Z",
     "start_time": "2018-06-06T01:43:17.069701Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scikitplot.metrics.plot_roc(Y_test[:,1], pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
